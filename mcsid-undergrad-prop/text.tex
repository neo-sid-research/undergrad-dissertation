\chapter{Introduction}
Our work aims to accomplish three objectives: to expand the original SID dataset with images from mobile devices, experiment with loss functions based on perceptual quality metrics to try and mitigate the L1's artifacts [ref that figure where solid colors are all messy], and make this algorithm easier to use for developers who are not deep learning researchers.

Our reasoning to believe those goals are worthy is as follows: 

Chen et al argue on \cite{DBLP:conf/cvpr/ChenCXK18} that "best results will be obtained when a dedicated network is trained for a specific camera sensor".
The latest smartphones allow for very fine control over exposure and ISO, which enables us to create a dataset in the same fashion as the SID dataset, but with mobile images.
What we are thinking here is, though mobile sensors are quite different from those of DSLRs, they are considerably close to each other (oof unscientific unstained claim based on common sense).
As Chen et al also noted "experiments with cross-sensor generalization indicate that [dedicated networks] may not always be necessary [if the sensors are similar enough]".
By making available a new dataset with images from mobile devices, more research will be possible.

Zhao et al showed the importance of perceptually-motivated losses for image restoration tasks where humans are the ones assessing the images' quality \cite{DBLP:journals/tci/ZhaoGFK17}.
Low-light image enhancing and denoising is exactly that kind of task.

By publishing a version of our code licensed as free software with an easy to use API, we will enable a broader audience to experiment with this technology, benefit from it, and maybe even contribute to its development, as we have seen happen with the Tesseract open-source OCR engine [cite their github or that Google paper].


\chapter{Background and Related Work}

LSID advantages over traditional pipelines.

LSID experiments with GAN losses

MAE loss shortcomings for this task.

SSIM advantages and disadvantages.

Where MS-SSIM comes in

Using both L1 and MS-SSIM as shown in \cite{DBLP:conf/miccai/RonnebergerFB15} allows each to compensate for the other's faults.


\chapter{Mobile Cameras: See-in-the-Dark dataset}
MCSID, pronounced McSeed, is a new dataset we collected inspired by the original SID dataset.
We photographed 600 hundred distinct settings, with as many different sensors as we can get our hands own (as long as they all output a Bayer matrix **footnote explaining**).-+.

\section{Images}
It contains several RAW short exposure images (usually very dark, noisy, and color inaccurate renditions), each paired with a reference RAW long exposure image.
All frames are aligned with each other, as the cameras used were on tripods.
In order to maximize the usefulness of our dataset, we took photos using several different exposures and, instead of a single taking singles, we took bursts.
That will enable us to better evaluate LSID's strategy against traditional burst-based denoising algorithms.
Furthermore, it will enable us and future researchers to create different approaches to the low-light imaging problem (e.g., composing N images in a single frame).

\section{Devices}
We used a Xiaomi Mi Mix S3, a Redmii and a Canon DSLR.
Here we will list their specs, which sensor they use, supported resolution, ISO, exposures, etc.

\section{Methods}
We took our photographs in controlled environments.
The devices were kept stable on a tripod.
We created a small Android application to take the photos automatically, so we would not need to touch our devices to set them up in between shots.


\chapter{Experiments}

\section{}
