\chapter{Introduction}
Our work aims to accomplish two big objectives:
enhance the LSID pipeline so it performs better (both qualitatively and quantitatively) and make this algorithm easier to use for developers who are not deep learning researchers.

In order to achieve that, we have come up with the following goals for our project:
to expand the original SID dataset with images from mobile devices;
to experiment with loss functions based on perceptual quality metrics to try and mitigate the L1's artifacts [ref that figure where solid colors are all messy];
to experiment with extra supervision, specifically different forms of classification and segmentation to augment the image-to-image translation;


Chen et al argue on \cite{DBLP:conf/cvpr/ChenCXK18:lsid} that "best results will be obtained when a dedicated network is trained for a specific camera sensor".
The latest smartphones allow for very fine control over exposure and ISO, which enables us to create a dataset in the same fashion as the SID dataset, but with mobile images.
What we are thinking here is, though mobile sensors are quite different from those of DSLRs, they are considerably close to each other (oof unscientific unstained claim based on common sense).
As Chen et al also noted "experiments with cross-sensor generalization indicate that [dedicated networks] may not always be necessary [if the sensors are similar enough]".
By making available a new dataset with images from mobile devices, more research will be possible.

Zhao et al showed the importance of perceptually-motivated losses for image restoration tasks where humans are the ones assessing the images' quality \cite{DBLP:journals/tci/ZhaoGFK17:l1ssimloss}.
Low-light image enhancing and denoising is exactly that kind of task.

By publishing a version of our code licensed as free software with an easy to use API, we will enable a broader audience to experiment with this technology, benefit from it, and maybe even contribute to its development, as we have seen happen with the Tesseract open-source OCR engine [cite their github or that Google paper].

\section{Structure}
The following chapters were written considering with the final submission in mind, therefore the past tense was often used even though when referencing events that have not happened yet.
We realize this may confuse the reviewer; there is a chart at the end outlining our plans to make it easier for the reader of this proposal to understand when which events will happen.


\chapter{Background and Related Work}
We will explore the relevant literature, emphasizing what 

\section{Learning to See in the Dark}
LSID advantages over traditional pipelines...

MAE loss shortcomings for this task...

SSIM advantages and disadvantages...

Where MS-SSIM comes in...

Using both L1 and MS-SSIM as shown in \cite{DBLP:journals/tci/ZhaoGFK17:l1ssimloss} allows each to compensate for the other's faults.


\chapter{Mobile Cameras: See-in-the-Dark dataset}
MCSID, pronounced "McSeed", is a new dataset we collected and made available inspired by the original SID dataset.
We photographed 600 hundred distinct settings, with as many different sensors as we can get our hands own (as long as they all output a Bayer matrix **footnote explaining**).

\section{Images}
MCSID contains several RAW short exposure images (usually very dark, noisy, and color inaccurate renditions), each paired with a reference RAW long exposure image.
All frames are aligned with each other, as the cameras used were on tripods.
In order to maximize the usefulness of our dataset, we took photos using several different exposures and, instead of a single taking singles, we took bursts.
That will enable us to better evaluate LSID's strategy against traditional burst-based denoising algorithms.
Furthermore, it will enable us and future researchers to create different approaches to the low-light imaging problem (e.g., composing N images in a single frame).

\section{Devices}
We used a Xiaomi Mi Mix S3 and a Redmii (which model?).
Here we will list their specs, which sensor they use, supported resolution, ISO, exposures, etc.

\subsection{Xiaomi Mi Mix S3}
[wip]

\subsection{Redmii}
[wip]

\section{Methods}
We took our photographs in controlled environments.
The devices were kept stable on a tripod.
We created a small Android application to take the photos automatically, so we would not need to touch our devices to set them up in between shots.


\chapter{Experiments}

\section{}


